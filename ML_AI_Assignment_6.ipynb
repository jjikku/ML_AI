{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X2xUu1z-YpSX"
      },
      "outputs": [],
      "source": [
        "# !pip install emot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2VjZ7WonQnl",
        "outputId": "e01ae458-b7ee-4000-a8da-049dbad92849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAA-aIEjZe-t",
        "outputId": "a1e48fde-0b04-48ab-e6a5-1e46b2fa07e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'label': ['Negative emotion' 'Positive emotion'\n",
            " 'No emotion toward brand or product' \"I can't tell\"]\n",
            "New label distribution:\n",
            " 2    5389\n",
            "0    2978\n",
            "1     570\n",
            "3     156\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lower case\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    # Replace URLs with a special token\n",
        "    text = re.sub(r'http\\S+', 'URL', text)\n",
        "    # Replace user @ references with a special token\n",
        "    text = re.sub(r'\\@\\w+', '@USER', text)\n",
        "    # Replace hashtags by just words (remove # but keep the word)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    # Replace numbers with a special token\n",
        "    text = re.sub(r'\\d+', 'NUMBER', text)\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation from each token\n",
        "    tokens = [t for t in tokens if t.isalpha()]\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if not t in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    # Join all tokens back to a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "file_path = '/content/judge-1377884607_tweet_product_company - judge-1377884607_tweet_product_company.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns for easier reference\n",
        "df.rename(columns={'tweet_text': 'tweet', 'is_there_an_emotion_directed_at_a_brand_or_product': 'label'}, inplace=True)\n",
        "\n",
        "# Apply preprocessing to the 'tweet' column\n",
        "df['tweet'] = df['tweet'].apply(preprocess_text)\n",
        "\n",
        "# Apply preprocessing to each tweet\n",
        "df['tweet'] = df['tweet'].apply(preprocess_text)\n",
        "\n",
        "print(\"Unique values in 'label':\", df['label'].unique())\n",
        "# Updated mapping to include all classes\n",
        "class_mapping = {\n",
        "    'Positive emotion': 0,\n",
        "    'Negative emotion': 1,\n",
        "    'No emotion toward brand or product': 2,  # Assuming this is the 'no_idea' class\n",
        "    \"I can't tell\": 3  # Assuming this is the 'neutral' class\n",
        "}\n",
        "\n",
        "# Apply the mapping to the label column\n",
        "df['label'] = df['label'].map(class_mapping)\n",
        "\n",
        "# Verify the new distribution of mapped labels\n",
        "print(\"New label distribution:\\n\", df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "-i4Qe4jEEg9R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'df' is your preprocessed DataFrame\n",
        "\n",
        "# Tokenizer creation and fitting\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['tweet'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['tweet'])\n",
        "\n",
        "# Padding sequences\n",
        "max_sequence_length = max(len(x) for x in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y = to_categorical(df['label'].values)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "embedding_dim = 50  # You can choose different sizes for the embedding dimension\n",
        "rnn_units = 64  # The number of units in the RNN layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(SimpleRNN(units=rnn_units, return_sequences=True))  # Return sequences to stack another RNN layer\n",
        "model.add(SimpleRNN(units=rnn_units))  # Additional RNN layer\n",
        "# You can add more layers here if needed\n",
        "model.add(Dense(8, activation='relu'))  # Additional Dense layer with ReLU activation\n",
        "model.add(Dense(y_train.shape[1], activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping]  # Add early stopping callback\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M_tyAEx33oH",
        "outputId": "97740e56-9f10-43fa-b333-1af732bd3fd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "205/205 [==============================] - 7s 21ms/step - loss: 0.9315 - accuracy: 0.5869 - val_loss: 0.8680 - val_accuracy: 0.6154\n",
            "Epoch 2/100\n",
            "205/205 [==============================] - 4s 19ms/step - loss: 0.7019 - accuracy: 0.7226 - val_loss: 0.9450 - val_accuracy: 0.6181\n",
            "Epoch 3/100\n",
            "205/205 [==============================] - 6s 27ms/step - loss: 0.3945 - accuracy: 0.8602 - val_loss: 1.0875 - val_accuracy: 0.6415\n",
            "Epoch 4/100\n",
            "205/205 [==============================] - 4s 19ms/step - loss: 0.2667 - accuracy: 0.9054 - val_loss: 1.1394 - val_accuracy: 0.6442\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.8847 - accuracy: 0.6031\n",
            "Test Accuracy: 0.6030786037445068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from numpy import average\n",
        "import numpy as np\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=-1)  # Now shape is (num_samples, max_sequence_length, 1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)  # Now shape is (num_samples, max_sequence_length, 1)\n",
        "\n",
        "# Define the LSTM model\n",
        "def build_lstm_model(input_shape, output_units):\n",
        "    lstm_input = Input(shape=input_shape)\n",
        "    lstm_layer = LSTM(units=rnn_units, return_sequences=False)(lstm_input)\n",
        "    dense_layer = Dense(8, activation='relu')(lstm_layer)\n",
        "    output_layer = Dense(output_units, activation='softmax')(dense_layer)\n",
        "    model = Model(inputs=lstm_input, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = build_lstm_model((max_sequence_length, 1), y_train.shape[1])\n",
        "\n",
        "# Train the LSTM model with early stopping\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Make predictions with both models\n",
        "simple_rnn_predictions = model.predict(X_test)\n",
        "lstm_predictions = lstm_model.predict(X_test)\n",
        "\n",
        "# Average the predictions from both models\n",
        "ensemble_predictions = average([simple_rnn_predictions, lstm_predictions], axis=0)\n",
        "\n",
        "# Convert ensemble predictions to label indices\n",
        "ensemble_label_indices = ensemble_predictions.argmax(axis=1)\n",
        "\n",
        "# Convert true labels from one-hot encoding to label indices\n",
        "true_label_indices = y_test.argmax(axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iNnCa13CGrCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate ensemble model\n",
        "ensemble_accuracy = (ensemble_label_indices == true_label_indices).mean()\n",
        "print(f'Ensemble Test Accuracy: {ensemble_accuracy}')\n",
        "\n",
        "# Generate a classification report\n",
        "print(confusion_matrix(true_label_indices, ensemble_label_indices))\n",
        "print(classification_report(true_label_indices, ensemble_label_indices))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR1rcS8tIGna",
        "outputId": "28ad2a40-fb94-45c8-8d02-6d450381a8a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Test Accuracy: 0.6091258933479934\n",
            "[[  59    0  530    0]\n",
            " [  15    0  100    0]\n",
            " [  34    0 1049    0]\n",
            " [   2    0   30    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.10      0.17       589\n",
            "           1       0.00      0.00      0.00       115\n",
            "           2       0.61      0.97      0.75      1083\n",
            "           3       0.00      0.00      0.00        32\n",
            "\n",
            "    accuracy                           0.61      1819\n",
            "   macro avg       0.29      0.27      0.23      1819\n",
            "weighted avg       0.54      0.61      0.50      1819\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLaj3-YzIKgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}