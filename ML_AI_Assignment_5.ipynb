{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXO2eZwRO3mW",
        "outputId": "f9e458cc-87f0-4c22-a664-2cb5c99706ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn9rck-RNX7O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to clean the tweet text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove excess whitespace\n",
        "    text = text.strip()\n",
        "    # Debug statement to check the preprocessing\n",
        "    print(f\"Preprocessed text: {text}\")\n",
        "    return text\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/tweets - tweets.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Apply preprocessing to the 'tweet' column\n",
        "df['tweet'] = df['tweet'].apply(preprocess_text)\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['label'], test_size=0.2)\n",
        "\n",
        "# Display the first few rows of the processed data\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_pTL6OVONbm",
        "outputId": "e2810621-6306-48c7-a7ea-e2e31d64c0da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(tweet)\n",
        "\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Rejoin tokens into a single string\n",
        "    clean_tweet = ' '.join(tokens)\n",
        "\n",
        "    # Debug statement\n",
        "    print(f\"Original: {tweet}\\nProcessed: {clean_tweet}\\n\")\n",
        "\n",
        "    return clean_tweet\n",
        "\n",
        "df['processed_tweet'] = df['tweet'].apply(preprocess_tweet)\n",
        "\n",
        "# Display the DataFrame to verify results\n",
        "print(df[['tweet', 'processed_tweet']])\n"
      ],
      "metadata": {
        "id": "bSfFZfulQ7KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbcFGvpFXeFZ",
        "outputId": "5901e703-7537-4b1e-a6ef-f264c43b2abc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download the model and return as object ready for use\n",
        "model_glove_twitter = api.load(\"glove-twitter-25\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1_TpSBrXfIt",
        "outputId": "7d829910-65db-47b2-e869-129e7c4921ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: BoW Vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(df['processed_tweet'])\n",
        "\n",
        "# Step 2: TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_tweet'])\n",
        "\n",
        "# Step 3: GloVe Word Embeddings\n",
        "def get_embedding_vector(tweet, model):\n",
        "    # Tokenize the tweet\n",
        "    words = tweet.split()\n",
        "\n",
        "    # Retrieve the vector for each word in the tweet and take the mean to represent the string of words\n",
        "    embeddings = [model[word] for word in words if word in model]\n",
        "    if embeddings:\n",
        "        embeddings = np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        # If the tweet contains no words with embeddings, return zeros\n",
        "        embeddings = np.zeros(model.vector_size)\n",
        "    return embeddings\n",
        "\n",
        "# Apply the function to each tweet to get the embedding vector\n",
        "df['embedding_vector'] = df['processed_tweet'].apply(lambda tweet: get_embedding_vector(tweet, model_glove_twitter))\n",
        "\n",
        "# Convert the list of vectors into a numpy matrix, which can be used as input to ML models\n",
        "embedding_matrix = np.array(list(df['embedding_vector']))\n"
      ],
      "metadata": {
        "id": "8wNwMiGDYAZd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "\n",
        "# 4. Splitting the Dataset\n",
        "# Concatenate BoW and TF-IDF features\n",
        "X_combined = hstack([X_bow, X_tfidf])\n",
        "y = df['label']\n",
        "\n",
        "# Split the combined features\n",
        "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the GloVe features\n",
        "X_train_glove, X_test_glove, _, _ = train_test_split(embedding_matrix, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Model Selection\n",
        "clf_combined = RandomForestClassifier(random_state=42)\n",
        "clf_glove = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 6. Model Training\n",
        "clf_combined.fit(X_train_combined, y_train)\n",
        "clf_glove.fit(X_train_glove, y_train)\n",
        "\n",
        "# 7. Model Evaluation\n",
        "# Predict using the combined BoW and TF-IDF model\n",
        "pred_combined = clf_combined.predict(X_test_combined)\n",
        "# Predict using the GloVe model\n",
        "pred_glove = clf_glove.predict(X_test_glove)\n",
        "\n",
        "# 8. Hyperparameter Tuning\n",
        "# Here we will use a simple grid search on the combined model as an example\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf_combined, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_combined, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_clf_combined = grid_search.best_estimator_\n",
        "\n",
        "# 9. Model Testing\n",
        "# Make predictions with the best model\n",
        "best_pred_combined = best_clf_combined.predict(X_test_combined)\n",
        "\n",
        "# Combine predictions from the two models (e.g., by averaging their prediction probabilities)\n",
        "final_pred = np.round((best_pred_combined + pred_glove) / 2).astype(int)\n",
        "\n",
        "# Calculate the final accuracy\n",
        "final_accuracy = accuracy_score(y_test, final_pred)\n",
        "print(f\"Final Accuracy: {final_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aihX69EQZrAh",
        "outputId": "33550b19-cf8c-4f02-e1ae-b9a79cbbfee9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Final Accuracy: 0.8465909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using ANN\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, concatenate\n",
        "from keras.utils import to_categorical\n",
        "from scipy.sparse import hstack\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assuming you have X_bow, X_tfidf, and embedding_matrix ready from the previous steps\n",
        "# Also assuming y is a numpy array of your labels\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_encoded = to_categorical(y)\n",
        "\n",
        "# 4. Splitting the Dataset\n",
        "# Concatenate BoW and TF-IDF features\n",
        "X_combined = hstack([X_bow, X_tfidf])\n",
        "\n",
        "# Split the combined features into train and test sets\n",
        "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the GloVe features into train and test sets\n",
        "X_train_glove, X_test_glove, _, _ = train_test_split(embedding_matrix, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the combined sparse matrix to a dense one for Keras\n",
        "X_train_combined = X_train_combined.todense()\n",
        "X_test_combined = X_test_combined.todense()\n",
        "\n",
        "# 5. Model Definition for Combined BoW and TF-IDF\n",
        "input_combined = Input(shape=(X_train_combined.shape[1],))\n",
        "dense_combined = Dense(64, activation='relu')(input_combined)\n",
        "dropout_combined = Dropout(0.5)(dense_combined)\n",
        "output_combined = Dense(y_encoded.shape[1], activation='softmax')(dropout_combined)\n",
        "model_combined = Model(inputs=input_combined, outputs=output_combined)\n",
        "\n",
        "# Model Definition for GloVe\n",
        "input_glove = Input(shape=(X_train_glove.shape[1],))\n",
        "dense_glove = Dense(64, activation='relu')(input_glove)\n",
        "dropout_glove = Dropout(0.5)(dense_glove)\n",
        "output_glove = Dense(y_encoded.shape[1], activation='softmax')(dropout_glove)\n",
        "model_glove = Model(inputs=input_glove, outputs=output_glove)\n",
        "\n",
        "# Compile the models\n",
        "model_combined.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_glove.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# 6. Model Training\n",
        "model_combined.fit(X_train_combined, y_train, validation_split=0.1, epochs=10, batch_size=32, callbacks=[early_stopping])\n",
        "model_glove.fit(X_train_glove, y_train, validation_split=0.1, epochs=10, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# 7. Model Evaluation\n",
        "pred_combined = model_combined.predict(X_test_combined)\n",
        "pred_glove = model_glove.predict(X_test_glove)\n",
        "\n",
        "# 9. Model Testing\n",
        "# Averaging predictions from both models\n",
        "final_pred = (pred_combined + pred_glove) / 2\n",
        "final_pred_labels = np.argmax(final_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test back to labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the final accuracy\n",
        "final_accuracy = np.mean(final_pred_labels == y_test_labels)\n",
        "print(f\"Final Accuracy: {final_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9MqK2R0cxMB",
        "outputId": "c529218e-2714-4c19-f537-3797ae37c04c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "179/179 [==============================] - 9s 44ms/step - loss: 0.3776 - accuracy: 0.8248 - val_loss: 0.2405 - val_accuracy: 0.9006\n",
            "Epoch 2/10\n",
            "179/179 [==============================] - 6s 35ms/step - loss: 0.1929 - accuracy: 0.9255 - val_loss: 0.2302 - val_accuracy: 0.8959\n",
            "Epoch 3/10\n",
            "179/179 [==============================] - 5s 29ms/step - loss: 0.1262 - accuracy: 0.9535 - val_loss: 0.2484 - val_accuracy: 0.8864\n",
            "Epoch 4/10\n",
            "179/179 [==============================] - 6s 36ms/step - loss: 0.0784 - accuracy: 0.9730 - val_loss: 0.2741 - val_accuracy: 0.8785\n",
            "Epoch 5/10\n",
            "179/179 [==============================] - 6s 31ms/step - loss: 0.0547 - accuracy: 0.9823 - val_loss: 0.2969 - val_accuracy: 0.8754\n",
            "Epoch 1/10\n",
            "179/179 [==============================] - 1s 4ms/step - loss: 0.4347 - accuracy: 0.7929 - val_loss: 0.3200 - val_accuracy: 0.8407\n",
            "Epoch 2/10\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3249 - accuracy: 0.8522 - val_loss: 0.2925 - val_accuracy: 0.8644\n",
            "Epoch 3/10\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3055 - accuracy: 0.8618 - val_loss: 0.2831 - val_accuracy: 0.8817\n",
            "Epoch 4/10\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.2975 - accuracy: 0.8658 - val_loss: 0.2800 - val_accuracy: 0.8880\n",
            "Epoch 5/10\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8672 - val_loss: 0.2783 - val_accuracy: 0.8833\n",
            "Epoch 6/10\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.2915 - accuracy: 0.8693 - val_loss: 0.2800 - val_accuracy: 0.8707\n",
            "Epoch 7/10\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.2903 - accuracy: 0.8679 - val_loss: 0.2811 - val_accuracy: 0.8754\n",
            "Epoch 8/10\n",
            "179/179 [==============================] - 1s 4ms/step - loss: 0.2840 - accuracy: 0.8765 - val_loss: 0.2781 - val_accuracy: 0.8817\n",
            "Epoch 9/10\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.2845 - accuracy: 0.8743 - val_loss: 0.2786 - val_accuracy: 0.8770\n",
            "Epoch 10/10\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.2893 - accuracy: 0.8714 - val_loss: 0.2766 - val_accuracy: 0.8817\n",
            "50/50 [==============================] - 1s 9ms/step\n",
            "50/50 [==============================] - 0s 1ms/step\n",
            "Final Accuracy: 0.889520202020202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbftoS_CcyIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}